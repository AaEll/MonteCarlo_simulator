{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from functools import reduce\n",
    "import random\n",
    "\n",
    "def Question1and3(sim, n):\n",
    "    sum1,sq_sum1,mini,maxi = reduce(lambda x,y : (x[0]+y,x[1]+y**2,min(x[2],y),max(x[3],y)),\\\n",
    "                                        sim.sample_repeated(n),(0,0,999999,-999999))\n",
    "    print(\"The mean is {}\".format(sum1/n))\n",
    "    print(\"The std_dev is {}\".format(math.sqrt(sq_sum1/n - (sum1/n)**2)))\n",
    "    print(\"The min is {}\".format(mini))\n",
    "    print(\"The max is {}\".format(maxi))\n",
    "def Question4(sim,boolean_func, n):\n",
    "    sum1 = reduce(lambda x,y :x + int(y), sim.sample_boolean_repeated(boolean_func,n),0)\n",
    "    print(\"The probability is {}\".format(sum1/n))\n",
    "\n",
    "    \n",
    "class MDP():\n",
    "    termial_absorbing_state = \"inf\"\n",
    "\n",
    "    \n",
    "    def __init__(self, states, actions, transition_function, reward_function, initial_state_func, damping_constant, policy):\n",
    "        self.states = states\n",
    "        self.actions = actions\n",
    "        self.transition_function = transition_function\n",
    "        self.reward_function = reward_function\n",
    "        self.initial_state_func = initial_state_func\n",
    "        self.damping_constant = damping_constant\n",
    "        self.policy = policy\n",
    "        \n",
    "        self.current_state = self.initial_state_func()\n",
    "        self.current_reward = 0\n",
    "        self.timestep = 0\n",
    "\n",
    "        \n",
    "    def run_step(self):\n",
    "        action = self.policy(self.current_state,actions)\n",
    "        new_state = self.transition_function(self.current_state,action)\n",
    "        self.current_reward += self.reward_function(new_state,action)*self.damping_constant**(self.timestep)\n",
    "        self.timestep +=1\n",
    "        self.current_state = new_state\n",
    "    \n",
    "    def run_mdp(self):\n",
    "        self.current_state = self.initial_state_func()\n",
    "        self.current_reward = 0\n",
    "        self.timestep = 0\n",
    "        \n",
    "        while self.current_state != self.termial_absorbing_state:\n",
    "            self.run_step()\n",
    "        return self.current_reward\n",
    "    \n",
    "    def sample_repeated(self,n):\n",
    "        i = n\n",
    "        while (i>0):\n",
    "            i-=1\n",
    "            yield self.run_mdp()\n",
    "            \n",
    "    def check_boolean(self,boolean_func):\n",
    "        self.current_state = self.initial_state_func()\n",
    "        self.timestep = 0\n",
    "        \n",
    "        while self.current_state != self.termial_absorbing_state:\n",
    "            self.run_step()            \n",
    "            boolean = boolean_func(self)\n",
    "            if boolean is None:\n",
    "                pass\n",
    "            else:\n",
    "                return boolean\n",
    "        return False\n",
    "    def sample_boolean_repeated(self,boolean_func,n):\n",
    "        i = n\n",
    "        while (i>0):\n",
    "            i-=1\n",
    "            yield self.check_boolean(boolean_func)\n",
    "\n",
    "states = list(range(22)).append('inf')\n",
    "actions = [\"AU\",\"AD\",\"AL\",\"AR\"]\n",
    "action_dict= {\"AU\":[.8,0,.05,.05,.1],\n",
    "              \"AL\":[.05,.05,.8,0,.1],\n",
    "              \"AR\":[.05,.05,0,.8,.1],\n",
    "              \"AD\":[0,.8,.05,.05,.1]}\n",
    "                #   U D L R C\n",
    "transition_dict = {1:[0,5,0,1,0],\n",
    "                   2:[0,5,-1,1,0],\n",
    "                   3:[0,5,-1,1,0],\n",
    "                   4:[0,5,-1,1,0],\n",
    "                   5:[0,5,-1,0,0],\n",
    "                   6:[-5,5,0,1,0],\n",
    "                   7:[-5,5,-1,1,0],\n",
    "                   8:[-5,0,-1,1,0],\n",
    "                   9:[-5,4,-1,1,0],\n",
    "                  10:[-5,4,-1,0,0],\n",
    "                  11:[-5,4,0,1,0],\n",
    "                  12:[-5,4,-1,0,0],\n",
    "                  13:[-4,4,0,1,0],\n",
    "                  14:[-4,4,-1,0,0],\n",
    "                  15:[-4,4,0,1,0],\n",
    "                  16:[-4,4,-1,0,0],\n",
    "                  17:[-4,5,0,1,0],\n",
    "                  18:[-4,None,-1,0,0],\n",
    "                  19:[-4,0,0,1,0],\n",
    "                  20:[-4,0,-1,1,0],\n",
    "                  21:[0,0,-1,1,0],\n",
    "                  22:[-5,0,-1,None,0]}\n",
    "def policy_random(state,actions):\n",
    "    return random.choice(actions)\n",
    "\n",
    "def transition_func(state,action):\n",
    "    direction = np.random.choice(list(range(5)),p = action_dict[action])\n",
    "    change = transition_dict[state][direction]\n",
    "    if change is None:\n",
    "        return 'inf'\n",
    "    else:\n",
    "        return state + change\n",
    "def reward_func(state,action):\n",
    "    if state ==21:\n",
    "        return -10\n",
    "    elif state == 'inf':\n",
    "        return 10\n",
    "    else:\n",
    "        return 0\n",
    "lambda : 1\n",
    "# Definition of policy for Question 2\n",
    "position_action_dict = {1:\"AR\",\n",
    "               2:\"AR\",\n",
    "               3:\"AR\",\n",
    "               4:\"AR\", ####\n",
    "               5:\"AD\",\n",
    "               6:\"AR\",\n",
    "               7:\"AR\",\n",
    "               8:\"AR\",\n",
    "               9:\"AD\", ####\n",
    "              10:\"AD\",\n",
    "              11:\"AU\",\n",
    "              12:\"AU\",\n",
    "              13:\"AD\",\n",
    "              14:\"AD\",\n",
    "              15:\"AU\",\n",
    "              16:\"AU\",\n",
    "              17:\"AD\",#####\n",
    "              18:\"AD\",\n",
    "              19:\"AU\",\n",
    "              20:\"AU\",\n",
    "              21:\"AR\",\n",
    "              22:\"AR\"}\n",
    "                #   U D L R\n",
    "def policy_optimal(state,actions):\n",
    "    return position_action_dict[state]\n",
    "\n",
    "def boolean_Question4(my_mdp):\n",
    "    if my_mdp.timestep == 11:\n",
    "        return (my_mdp.current_state == 21)\n",
    "    else:\n",
    "        return None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean is -0.6051477003934808\n",
      "The std_dev is 2.2490168494556855\n",
      "The min is -27.533293644885067\n",
      "The max is 4.7829690000000005\n"
     ]
    }
   ],
   "source": [
    "# Answer to Question 1\n",
    "random_policyMDP = MDP(states,actions,transition_func,reward_func,lambda : 1, .9, policy_random)\n",
    "Question1and3(random_policyMDP,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean is 3.9491997845425204\n",
      "The std_dev is 0.668835620677968\n",
      "The min is 1.0941898913151242\n",
      "The max is 4.7829690000000005\n"
     ]
    }
   ],
   "source": [
    "# Answer to Question 3\n",
    "optimal_policyMDP = MDP(states,actions,transition_func,reward_func,lambda : 1, .9, policy_optimal)\n",
    "Question1and3(optimal_policyMDP,10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probability is 0.01872\n"
     ]
    }
   ],
   "source": [
    "# Answer to Question 4\n",
    "random_policyMDP = MDP(states,actions,transition_func,reward_func,lambda : 18, .9, policy_random)\n",
    "Question4(random_policyMDP,boolean_Question4,100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
